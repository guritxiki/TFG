{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from  customDatasetFromCSV import CustomDatasetFromCSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DATASET AND GET CUDA DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_csv= './filesCSV/captions.csv'\n",
    "vocab_csv ='./filesCSV/vocab.csv'\n",
    "data_dir = \"./imagesTrainVal/train2017/\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),       \n",
    "     \n",
    "])\n",
    "dataset = CustomDatasetFromCSV(data_dir,captions_csv,vocab_csv,transform=transform,percentage=100)\n",
    "len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capacidad de la GPU:\", torch.cuda.get_device_capability(0))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ARQUITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EncoderCNN\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, device):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        from torchvision.models.resnet import resnet50\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        self.device = device\n",
    "\n",
    "        # Disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules).to(self.device)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size).to(self.device)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_dim = attention_dim\n",
    "        self.W = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.A = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, features, hidden_state):\n",
    "        u_hs = self.U(features)  # (batch_size, num_layers, attention_dim)\n",
    "        w_ah = self.W(hidden_state)  # (batch_size, attention_dim)\n",
    "\n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1))  # (batch_size, num_layers, attention_dim)\n",
    "\n",
    "        attention_scores = self.A(combined_states)  # (batch_size, num_layers, 1)\n",
    "        attention_scores = attention_scores.squeeze(2)  # (batch_size, num_layers)\n",
    "\n",
    "        alpha = torch.softmax(attention_scores, dim=1)  # (batch_size, num_layers)\n",
    "\n",
    "        attention_weights = features * alpha.unsqueeze(2)  # (batch_size, num_layers, features_dim)\n",
    "        attention_weights = attention_weights.sum(dim=1)  # (batch_size, features_dim)\n",
    "\n",
    "        return alpha, attention_weights\n",
    "\n",
    "# DecoderRNN\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, caption_length, num_layers, attention_dim, encoder_dim, device):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        self.caption_length = caption_length\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size).to(device)\n",
    "        self.attention = Attention(encoder_dim, hidden_size, attention_dim).to(device)\n",
    "        self.lstm = nn.LSTM(embed_size + encoder_dim*2, hidden_size, num_layers, batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size).to(device)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device),\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        batch_size = features.size(0)\n",
    "        hidden = self.init_hidden(batch_size, self.device)\n",
    "        outputs = []\n",
    "\n",
    "   \n",
    "        input_tokens = torch.tensor([1], device=self.device).expand(batch_size).unsqueeze(1)\n",
    "\n",
    "        for t in range(self.caption_length):\n",
    "            embeddings = self.embed(input_tokens)\n",
    "            # print(f\" Embeddings shape: {embeddings.shape}\")\n",
    "            alpha, context = self.attention(features, hidden[0][0])  # Use the first hidden state\n",
    "            # print(f\" Context shape: {context.shape}\")\n",
    "            # print (f\" Features shape: {features.shape}\")\n",
    "            # featuresAndContext = features + context\n",
    "            featuresAndContext = torch.cat((features, context), dim=1)\n",
    "            # print(f\" FeaturesAndContext shape : {featuresAndContext.shape}\")\n",
    "          \n",
    "            featuresAndContext = featuresAndContext.unsqueeze(1)\n",
    "            # print(f\" FeaturesAndContext shape after unesqueezing : {featuresAndContext.shape}\")\n",
    "            lstm_input = torch.cat((embeddings, featuresAndContext), dim=2)  # Concatenate along the second dimension\n",
    "            # print(f\" LSTM input shape: {lstm_input.shape}\")\n",
    "            h, hidden = self.lstm(lstm_input, hidden)\n",
    "            # print(f\" H shape: {h.shape}\")\n",
    "            output = self.linear(h)\n",
    "            # print(f\" Output shape: {output.shape}\")\n",
    "            outputs.append(output)\n",
    "\n",
    "            input_tokens = captions[:, t].unsqueeze(1).to(self.device)\n",
    "          \n",
    "            # predicted_word_idx = output.argmax(dim=2)\n",
    "            \n",
    "            # input_tokens = predicted_word_idx.to(self.device)\n",
    "\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "    def generate(self, features):\n",
    "        input_token = torch.tensor([1], device=self.device).expand(features.size(0)).unsqueeze(1)\n",
    "        alphas = []\n",
    "        outputs = []\n",
    "        tokens= []\n",
    "        # Initialize the hidden state\n",
    "        hidden = self.init_hidden(features.size(0), self.device)\n",
    "\n",
    "        for _ in range(self.caption_length):\n",
    "            embeddings = self.embed(input_token)\n",
    "            # print(f\" Embeddings shape: {embeddings.shape}\")\n",
    "            # print(f\" Features shape: {features.shape}\")\n",
    "            \n",
    "            alpha, context = self.attention(features, hidden[0][0])\n",
    "            # alphas.append(alpha.cpu().detach().numpy())\n",
    "            alphas.append(alpha)\n",
    "            # print(f\" Alpha shape: {alpha.shape}\")\n",
    "            # print(f\" Context shape: {context.shape}\")\n",
    "            # print (f\" Features shape: {features.shape}\")\n",
    "            featuresAndContext = torch.cat((features, context), dim=1)\n",
    "\n",
    "            # featuresAndContext = features + context\n",
    "            # print(f\" FeaturesAndContext shape : {featuresAndContext.shape}\")\n",
    "            featuresAndContext = featuresAndContext.unsqueeze(1)\n",
    "            # print(f\" FeaturesAndContext shape after unesqueezing : {featuresAndContext.shape}\")\n",
    "            lstm_input = torch.cat((embeddings, featuresAndContext), dim=2)\n",
    "            h, hidden = self.lstm(lstm_input, hidden)\n",
    "            output = self.linear(h)\n",
    "            # print(f\"Output Shape: {output.shape}\") \n",
    "            outputs.append(output)\n",
    "\n",
    "            predicted_word_idx = output.argmax(dim=2)\n",
    "            \n",
    "            input_token = predicted_word_idx.to(self.device)\n",
    "            tokens.append(input_token)\n",
    "            # print(\"Input token shape : \",input_token.shape)\n",
    "            if predicted_word_idx[0, 0].item() == 2:\n",
    "\n",
    "\n",
    "                break\n",
    "            # print(\"Shape of input token\",input_token.shape)\n",
    "\n",
    "            # hidden = (h, c)  # Update hidden state for the next time step\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        tokens = torch.cat(tokens, dim=1)\n",
    "        \n",
    "        # print(f\"tokens shape: {tokens.shape}\")\n",
    "        # print(outputs.shape)\n",
    "        return tokens, alphas\n",
    "\n",
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, caption_length, num_layers, attention_dim, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, device)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, caption_length, num_layers, attention_dim, embed_size, device)\n",
    "        self.encoder.to(device)\n",
    "        self.decoder.to(device)\n",
    "\n",
    "        # Store arguments as object attributes\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.caption_length = caption_length\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_dim = attention_dim\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, image):\n",
    "        features = self.encoder(image)\n",
    "        max_len = self.caption_length\n",
    "        return self.decoder.generate(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 0.001\n",
    "max_caption_length = dataset.maxCaptionLength\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(embed_size, hidden_size, vocab_size, dataset.maxCaptionLength, num_layers, embed_size, device)  # Remove the duplicated hidden_size argument\n",
    "model.to(device) \n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create dataLoaders\n",
    "batch_size = 64  # Set your batch size\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data))))\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data), len(dataset.train_data) + len(dataset.val_data))))\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size * 6, sampler=SubsetRandomSampler(range(len(dataset.train_data) + len(dataset.val_data), len(dataset))))\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUMBER OF TRAINABLE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate the number of trainable parameters\n",
    "num_params = count_parameters(model)\n",
    "print(f'Number of trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(image, alphas, caption, num_boxes=8):\n",
    "    num_timesteps = len(caption)\n",
    "    img_height, img_width, _ = image.shape\n",
    "\n",
    "    # Create subplots for the original image and attention boxes\n",
    "    fig, axes = plt.subplots(2, (num_timesteps + 1) // 2, figsize=(15, 7))\n",
    "\n",
    "    # Display the original image\n",
    "    axes[0, 0].imshow(image.transpose(1, 2, 0))\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    axes[0, 0].axis('off')\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        temp_att = alphas[t].cpu().numpy()  # Move the tensor to CPU\n",
    "\n",
    "        # Check if the attention map has a valid shape\n",
    "        if temp_att.ndim == 1 and temp_att.shape[0] > 0:\n",
    "            # Normalize attention scores\n",
    "            attention_scores = (temp_att - temp_att.min()) / (temp_att.max() - temp_att.min())\n",
    "\n",
    "            # Divide the image into num_boxes x num_boxes boxes\n",
    "            box_height = img_height // num_boxes\n",
    "            box_width = img_width // num_boxes\n",
    "\n",
    "            # Create a copy of the original image\n",
    "            img_copy = np.copy(image.transpose(1, 2, 0))\n",
    "\n",
    "            # Iterate over each box and adjust transparency based on attention\n",
    "            for i in range(num_boxes):\n",
    "                for j in range(num_boxes):\n",
    "                    start_row, end_row = i * box_height, (i + 1) * box_height\n",
    "                    start_col, end_col = j * box_width, (j + 1) * box_width\n",
    "\n",
    "                    # Adjust transparency based on attention score\n",
    "                    img_copy[start_row:end_row, start_col:end_col, :] *= attention_scores[i * num_boxes + j]\n",
    "\n",
    "            # Display the image with attention overlay\n",
    "            axes[t // ((num_timesteps + 1) // 2), t % ((num_timesteps + 1) // 2)].set_title(caption[t])\n",
    "            img = axes[t // ((num_timesteps + 1) // 2), t % ((num_timesteps + 1) // 2)].imshow(img_copy)\n",
    "\n",
    "            # Add attention overlay\n",
    "            axes[t // ((num_timesteps + 1) // 2), t % ((num_timesteps + 1) // 2)].imshow(\n",
    "                attention_scores.reshape((num_boxes, num_boxes)),\n",
    "                cmap='gray', alpha=0.7, extent=img.get_extent()\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Attention map at timestep {t} has an unexpected shape: {temp_att.shape}\")\n",
    "            axes[t // ((num_timesteps + 1) // 2), t % ((num_timesteps + 1) // 2)].set_title(caption[t])\n",
    "            axes[t // ((num_timesteps + 1) // 2), t % ((num_timesteps + 1) // 2)].imshow(image.transpose(1, 2, 0))\n",
    "\n",
    "        axes[t // ((num_timesteps + 1) // 2), t % ((num_timesteps + 1) // 2)].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "patience = 3\n",
    "def train_image_captioning_model(model, optimizer, criterion, train_loader, val_loader, save_dir, num_epochs=10, start_epoch=0):\n",
    "    model.to(device)\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        batch_counter = 0  # Initialize batch counter\n",
    "        total_batches = len(train_loader)  # Total number of batches in the training set\n",
    "        train_losses = []\n",
    "\n",
    "        for images, captions in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "            # Move data to the appropriate device\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, captions)\n",
    "\n",
    "            # Pad captions to ensure they have the same length\n",
    "            padded_captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs.view(-1, len(dataset.vocab)), padded_captions.view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            batch_counter += 1\n",
    "\n",
    "            # Print loss and generated output every x batches\n",
    "            if batch_counter % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Batch [{batch_counter}/{total_batches}] - Loss: {loss.item():.4f}\")\n",
    "                    generated_output_ids = outputs.argmax(dim=2).tolist()\n",
    "                    generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids[0] if token_id != 0]\n",
    "                    generated_caption_str = \" \".join(generated_words)\n",
    "                    real_caption_ids = captions.tolist()\n",
    "                    real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids[0] if token_id != 0]\n",
    "                    real_caption_str = \" \".join(real_caption_words)\n",
    "                    print(f\"Generated Output: {generated_caption_str}\")\n",
    "                    print(f\"Real Caption: {real_caption_str}\")\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for images, captions in val_loader:\n",
    "                # Move data to the appropriate device\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "\n",
    "                outputs = model(images, captions)\n",
    "\n",
    "                # Pad captions to ensure they have the same length\n",
    "                padded_captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "\n",
    "                # Calculate the loss\n",
    "                val_loss = criterion(outputs.view(-1, len(dataset.vocab)), padded_captions.view(-1))\n",
    "                val_losses.append(val_loss.item())\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f'model_epoch_{epoch + 1}_val_loss_{avg_val_loss:.4f}.pt'))\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Validation loss hasn't improved for {patience} epochs. Early stopping...\")\n",
    "            break\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "# Example usage:\n",
    "train_image_captioning_model(model, optimizer, criterion, train_loader, val_loader, save_dir, patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Define the directory where you want to save the model\n",
    "model_path = '.\\SAVED_MODELS\\model2\\model_epoch_1_val_loss_2.3506.pt'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Create a CSV file to store real and generated captions\n",
    "csv_file = 'resultsModel2Curriculum.csv'\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Real Caption', 'Generated Caption'])\n",
    "\n",
    "# Testing loop on the test dataset\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_counter = 0  # Initialize batch counter\n",
    "    total_batches = len(test_loader)  # Total number of batches in the training set\n",
    "    for images, captions in tqdm(test_loader, desc='Testing'):\n",
    "        # Move data to the appropriate device\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        outputs, alphas = model.generate(images)  # Assuming your model generates captions given images\n",
    "        if batch_counter % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                image = images[0].cpu().numpy()  #\n",
    "                image = np.transpose(image, (1, 2, 0))\n",
    "                plt.imshow(image)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                plt.show()\n",
    "                generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in outputs[0].tolist() if token_id != 0]\n",
    "                generated_caption_str = \" \".join(generated_words)\n",
    "                real_caption_ids = captions.tolist()\n",
    "                real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids[0] if token_id != 0]\n",
    "                real_caption_str = \" \".join(real_caption_words)\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}] - Batch [{batch_counter}/{total_batches}] - Loss: {loss.item():.4f}\")\n",
    "                print(f\"Generated Output: {generated_caption_str}\")\n",
    "                print(f\"Real Caption: {real_caption_str}\")\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in outputs[i].tolist() if token_id != 0]\n",
    "            generated_caption_str = \" \".join(generated_words)\n",
    "            real_caption_ids = captions.tolist()\n",
    "            real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids[i] if token_id != 0]\n",
    "            real_caption_str = \" \".join(real_caption_words)\n",
    "\n",
    "            # Append the real and generated captions to the CSV file\n",
    "            with open(csv_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([real_caption_str, generated_caption_str])\n",
    "        batch_counter+=1\n",
    "print(\"Testing completed. Real and generated captions saved in 'results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = './resultsModel2Curriculum.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Lists to store metric scores\n",
    "bleu_scores = []\n",
    "rouge_l_scores = []\n",
    "\n",
    "# Smoothing function for BLEU score\n",
    "smoother = SmoothingFunction().method1\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    real_caption = \" \".join(word_tokenize(row['Real Caption']))  # Join the words into a string\n",
    "    generated_caption = \" \".join(word_tokenize(row['Generated Caption']))  # Join the words into a string\n",
    "\n",
    "    real_caption_tokenized = word_tokenize(real_caption)\n",
    "    generated_caption_tokenized = word_tokenize(generated_caption)\n",
    "    \n",
    "    # BLEU Score\n",
    "    bleu = sentence_bleu([real_caption], generated_caption, smoothing_function=smoother)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # ROUGE-L Score\n",
    "    rouge_scores = scorer.score(real_caption, generated_caption)\n",
    "    rouge_l_f1 = rouge_scores['rougeL'].fmeasure\n",
    "    rouge_l_scores.append(rouge_l_f1)\n",
    "\n",
    "# Add metric columns to the DataFrame\n",
    "df['BLEU Score'] = bleu_scores\n",
    "df['ROUGE-L Score'] = rouge_l_scores\n",
    "\n",
    "# Calculate mean scores\n",
    "mean_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "mean_rouge_l_score = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "# Print the DataFrame with scores and mean scores\n",
    "print(df)\n",
    "print(\"\\nMean Metrics:\")\n",
    "print(f\"Mean BLEU Score: {mean_bleu_score:.4f}\")\n",
    "print(f\"Mean ROUGE-L Score: {mean_rouge_l_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = './resultsCSV/resultsModel2.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Lists to store metric scores\n",
    "bleu_scores = []\n",
    "rouge_l_scores = []\n",
    "\n",
    "# Smoothing function for BLEU score\n",
    "smoother = SmoothingFunction().method1\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    real_caption = \" \".join(word_tokenize(row['Real Caption']))  # Join the words into a string\n",
    "    generated_caption = \" \".join(word_tokenize(row['Generated Caption']))  # Join the words into a string\n",
    "\n",
    "    real_caption_tokenized = word_tokenize(real_caption)\n",
    "    generated_caption_tokenized = word_tokenize(generated_caption)\n",
    "    \n",
    "    # BLEU Score\n",
    "    bleu = sentence_bleu([real_caption], generated_caption, smoothing_function=smoother)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # ROUGE-L Score\n",
    "    rouge_scores = scorer.score(real_caption, generated_caption)\n",
    "    rouge_l_f1 = rouge_scores['rougeL'].fmeasure\n",
    "    rouge_l_scores.append(rouge_l_f1)\n",
    "\n",
    "# Add metric columns to the DataFrame\n",
    "df['BLEU Score'] = bleu_scores\n",
    "df['ROUGE-L Score'] = rouge_l_scores\n",
    "\n",
    "# Calculate mean scores\n",
    "mean_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "mean_rouge_l_score = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "# Print the DataFrame with scores and mean scores\n",
    "print(df)\n",
    "print(\"\\nMean Metrics:\")\n",
    "print(f\"Mean BLEU Score: {mean_bleu_score:.4f}\")\n",
    "print(f\"Mean ROUGE-L Score: {mean_rouge_l_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = './resultsCSV/resultsModel2Encoder.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Lists to store metric scores\n",
    "bleu_scores = []\n",
    "rouge_l_scores = []\n",
    "\n",
    "# Smoothing function for BLEU score\n",
    "smoother = SmoothingFunction().method1\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    real_caption = \" \".join(word_tokenize(row['Real Caption']))  # Join the words into a string\n",
    "    generated_caption = \" \".join(word_tokenize(row['Generated Caption']))  # Join the words into a string\n",
    "\n",
    "    real_caption_tokenized = word_tokenize(real_caption)\n",
    "    generated_caption_tokenized = word_tokenize(generated_caption)\n",
    "    \n",
    "    # BLEU Score\n",
    "    bleu = sentence_bleu([real_caption], generated_caption, smoothing_function=smoother)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # ROUGE-L Score\n",
    "    rouge_scores = scorer.score(real_caption, generated_caption)\n",
    "    rouge_l_f1 = rouge_scores['rougeL'].fmeasure\n",
    "    rouge_l_scores.append(rouge_l_f1)\n",
    "\n",
    "# Add metric columns to the DataFrame\n",
    "df['BLEU Score'] = bleu_scores\n",
    "df['ROUGE-L Score'] = rouge_l_scores\n",
    "\n",
    "# Calculate mean scores\n",
    "mean_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "mean_rouge_l_score = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "# Print the DataFrame with scores and mean scores\n",
    "print(df)\n",
    "print(\"\\nMean Metrics:\")\n",
    "print(f\"Mean BLEU Score: {mean_bleu_score:.4f}\")\n",
    "print(f\"Mean ROUGE-L Score: {mean_rouge_l_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "model_path= \"./SAVED_MODELS/model2/ModelEncoder2_epoch_4_Loss_2.2639.pth\"\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 0.001\n",
    "max_caption_length = dataset.maxCaptionLength\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(embed_size, hidden_size, vocab_size, dataset.maxCaptionLength, num_layers, embed_size, device)  # Remove the duplicated hidden_size argument\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_image_cv2(image_path):\n",
    "    # Load and preprocess the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to (224, 224)\n",
    "    image = image / 255.0  # Normalize pixel values to the range [0, 1]\n",
    "    image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "# Function to load and preprocess the image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "def generate_caption(model, image_path, id_to_token, real_description=None):\n",
    "    # Load and preprocess the image using OpenCV\n",
    "    image = load_and_preprocess_image(image_path)\n",
    "\n",
    "    # Move the model and image to the same device\n",
    "    device = next(model.parameters()).device\n",
    "    image = image.to(device)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the generated tokens and attention scores\n",
    "        generated_tokens, attention_scores = model.generate(image)\n",
    "\n",
    "        # Convert token IDs to words\n",
    "        generated_words = [id_to_token.get(token_id.item(), \"UNK\") for token_id in generated_tokens[0] if token_id != 0]\n",
    "        generated_caption_str = \" \".join(generated_words)\n",
    "\n",
    "    print(\"Generated description by the model:\", \"###  \", generated_caption_str, \"  ###\")\n",
    "    if real_description:\n",
    "        print(f\"Real description made by hand by the tester: ###  {real_description}  ###\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop over each image in the folder\n",
    "folder_path = './ImagenesPrueba/'\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename:#.endswith(\".jpg\")\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Print the image filename\n",
    "        print(f\"Image: {filename}\")\n",
    "\n",
    "        # Use the generate_caption function\n",
    "        generate_caption(model, image_path, dataset.id_to_token)\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
