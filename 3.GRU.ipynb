{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from  customDatasetFromCSV import CustomDatasetFromCSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DATASET AND GET CUDA DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_csv= './filesCSV/captions.csv'\n",
    "vocab_csv ='./filesCSV/vocab.csv'\n",
    "data_dir = \"./imagesTrainVal/train2017/\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),       \n",
    "     \n",
    "])\n",
    "dataset = CustomDatasetFromCSV(data_dir,captions_csv,vocab_csv,transform=transform,percentage=100)\n",
    "len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capacidad de la GPU:\", torch.cuda.get_device_capability(0))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ARQUITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, device):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        from torchvision.models.resnet import resnet50\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        self.device = device\n",
    "        # Disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules).to(self.device)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size).to(self.device)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# --------- Decoder ----------\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, caption_length, num_layers, device):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        self.caption_length = caption_length\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        # Embedding layer to convert token IDs to embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size).to(self.device)\n",
    "        # GRU layer that takes embeddings and hidden states as input\n",
    "        self.gru = nn.GRU(embed_size + embed_size, hidden_size, num_layers, batch_first=True).to(self.device)\n",
    "        # Linear layer to produce the vocabulary distribution\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size).to(self.device)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state.\n",
    "        \"\"\"\n",
    "        # Create initial hidden state filled with zeros\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=self.device)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Embed the start token\n",
    "        input_token = torch.tensor([0], device=self.device) \n",
    "        input_token = input_token.expand(features.size(0)).unsqueeze(1)\n",
    "        # Initialize the hidden state for the beginning of the sequence\n",
    "        hidden = self.init_hidden(features.size(0))\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.caption_length):\n",
    "            embeddings = self.embed(input_token)\n",
    "            #print(\" Features shape: \"+ str(features.shape))\n",
    "            #print(f\" Embeddings shape: {embeddings.shape}\")\n",
    "            inputs = torch.cat((features.unsqueeze(1), embeddings), dim=2)\n",
    "            #print(f\" inputs shape: {inputs.shape}\")\n",
    "\n",
    "            \n",
    "            gru_out, hidden = self.gru(inputs, hidden)\n",
    "            #print(f\" gru_out shape: {gru_out.shape}\")\n",
    "            \n",
    "            output = self.linear(gru_out.squeeze(1))\n",
    "            #print(f\" output shape: {output.shape}\")\n",
    "            output = output.unsqueeze(1)\n",
    "            #print(f\" output after unezquezee shape: {output.shape}\")\n",
    "            \n",
    "            \n",
    "            input_token = captions[:, t].unsqueeze(1).to(self.device)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # Concatenate the output tensors along the sequence dimension\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, features):\n",
    "        \"\"\"\n",
    "        Generate captions for the given image features.\n",
    "        Args:\n",
    "            features: Output from the encoder (image features)\n",
    "\n",
    "        Returns:\n",
    "            outputs: Predicted token scores (before softmax)\n",
    "        \"\"\"\n",
    "        # Embed the start token\n",
    "        input_token = torch.tensor([0], device=self.device)  # Start token\n",
    "        input_token = input_token.expand(features.size(0)).unsqueeze(1).to(self.device)  # Move to the correct device\n",
    "\n",
    "        # Initialize the hidden state for the beginning of the sequence\n",
    "        hidden = self.init_hidden(features.size(0))\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(self.caption_length):  # Fixed caption length\n",
    "            embeddings = self.embed(input_token)\n",
    "            #print(\" Features shape: \"+ str(features.shape))\n",
    "            #print(f\" Embeddings shape: {embeddings.shape}\")\n",
    "            # Combine the image features and embedded captions as input\n",
    "            inputs = torch.cat((features.unsqueeze(1), embeddings), dim=2)\n",
    "            #print(f\" inputs shape: {inputs.shape}\")\n",
    "            # Pass the inputs and hidden states through the GRU\n",
    "            gru_out, hidden = self.gru(inputs, hidden)\n",
    "            #print(f\" gru_out shape: {gru_out.shape}\")\n",
    "            # Pass the GRU outputs through the linear layer\n",
    "            output = self.linear(gru_out)\n",
    "            #print(f\" output shape: {output.shape}\")\n",
    "\n",
    "            # Update the input token for the next time step\n",
    "            input_token = output.argmax(2).squeeze(1).to(self.device)  # Use argmax to get the next token\n",
    "            #print(f\" input token shape: {input_token.shape}\")\n",
    "            input_token = input_token.unsqueeze(1)  # Add the sequence dimension back\n",
    "            #print(f\" input token despues unezqeeze shape: {input_token.shape}\")\n",
    "\n",
    "            # Handle end token and padding token\n",
    "            input_token[input_token == 2] = 0  # Replace end token with padding token\n",
    "            outputs.append(output)\n",
    "                \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, caption_length, num_layers, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, device)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, caption_length, num_layers, device)\n",
    "        self.encoder.to(device)\n",
    "        self.decoder.to(device)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, image):\n",
    "        features = self.encoder(image)\n",
    "        return self.decoder.generate(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 0.001\n",
    "max_caption_length = dataset.maxCaptionLength\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(embed_size, hidden_size, vocab_size, max_caption_length, num_layers, device)  # Correct model initialization\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0,) #first try without ignore_index=0\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create dataLoaders\n",
    "batch_size = 64  # Set your batch size\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data))))\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data), len(dataset.train_data) + len(dataset.val_data))))\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size * 6, sampler=SubsetRandomSampler(range(len(dataset.train_data) + len(dataset.val_data), len(dataset))))\n",
    "\n",
    "len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUMBER OF TRAINABLE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model.to(device)\n",
    "total_params = count_parameters(model)\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(search_for_existing_model, save_dir, embed_size, hidden_size, num_layers, vocab_size, learning_rate, max_caption_length, dataset, device, num_epochs=5, patience=3):\n",
    "    # Initialize model\n",
    "    model = Model(embed_size, hidden_size, vocab_size, max_caption_length, num_layers, device).to(device)\n",
    "\n",
    "    start_epoch = 0\n",
    "    if search_for_existing_model:\n",
    "        print(\"Looking for model...\")\n",
    "        model_files = os.listdir(save_dir)\n",
    "        best_model_path = None\n",
    "        lowest_eval_loss = float('inf')\n",
    "        for model_file in model_files:\n",
    "            eval_loss_str = model_file.split('_')[-1].split('.pt')[0]\n",
    "            try:\n",
    "                eval_loss = float(eval_loss_str)\n",
    "            except ValueError:\n",
    "                eval_loss = float('inf')\n",
    "            if eval_loss < lowest_eval_loss:\n",
    "                lowest_eval_loss = eval_loss\n",
    "                best_model_path = os.path.join(save_dir, model_file)\n",
    "\n",
    "        if best_model_path is not None:\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "            start_epoch = int(best_model_path.split('_epoch_')[1].split('_Loss')[0])\n",
    "            print(f\"Loaded the best model from: {best_model_path}, start_epoch: {start_epoch}\")\n",
    "        else:\n",
    "            print(\"No best model found in the directory.\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    '''\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data))))\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data), len(dataset.train_data) + len(dataset.val_data))))\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size * 6, sampler=SubsetRandomSampler(range(len(dataset.train_data) + len(dataset.val_data), len(dataset))))\n",
    "    '''\n",
    "    # Training loop with early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        batch_counter = 0\n",
    "        total_batches = len(train_loader)\n",
    "        train_losses = []\n",
    "\n",
    "        for images, captions in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, captions)\n",
    "            loss = F.cross_entropy(outputs.view(-1, vocab_size), captions.contiguous().view(-1), ignore_index=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter % 500== 0:\n",
    "                with torch.no_grad():\n",
    "                    image = images[0].cpu().numpy()\n",
    "                    image = np.transpose(image, (1, 2, 0))\n",
    "                    plt.imshow(image)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    print(\"################### FORWARD METHOD ################### \")\n",
    "                    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Batch [{batch_counter}/{total_batches}] - Loss: {loss.item():.4f}\")\n",
    "                    print(\"SHAPE OF OUTPUTS:\"+str(outputs.shape))\n",
    "                    generated_output_ids = outputs.argmax(dim=2).tolist()\n",
    "                    generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids[0] if token_id != 0]\n",
    "                    generated_caption_str = \" \".join(generated_words)\n",
    "                    real_caption_ids = captions.tolist()\n",
    "                    real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids[0] if token_id != 0]\n",
    "                    real_caption_str = \" \".join(real_caption_words)\n",
    "                    print(f\"Generated Output: {generated_caption_str}\")\n",
    "                    print(f\"Real Caption: {real_caption_str}\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    print(\"################### GENERATE METHOD ################### \")\n",
    "                    generated_outputs = model.generate(images)\n",
    "                    generated_output_ids = generated_outputs.argmax(dim=2).tolist()\n",
    "                    generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids[0] if token_id != 0]\n",
    "                    generated_caption_str = \" \".join(generated_words)\n",
    "                    real_caption_ids = captions.tolist()\n",
    "                    real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids[0] if token_id != 0]\n",
    "                    real_caption_str = \" \".join(real_caption_words)\n",
    "                    print(f\"Generated Output (Generated): {generated_caption_str}\")\n",
    "                    print(f\"Real Caption: {real_caption_str}\")\n",
    "                \n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        model_path = os.path.join(save_dir, f'Model3_epoch_{epoch + 1}_2024_06_09_{avg_train_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for images, captions in val_loader:\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                outputs = model(images, captions)\n",
    "                val_loss = criterion(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "# Usage\n",
    "train_model(\n",
    "    search_for_existing_model=False,\n",
    "    save_dir='./SAVED_MODELS/model3/',\n",
    "    embed_size=256,\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    vocab_size=len(dataset.vocab),\n",
    "    learning_rate=0.001,\n",
    "    max_caption_length=dataset.maxCaptionLength,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    num_epochs=5,\n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOOSING MODEL TO TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 0.001\n",
    "max_caption_length = dataset.maxCaptionLength\n",
    "model = Model(embed_size, hidden_size, vocab_size, max_caption_length, num_layers, device)  # Correct model initialization\n",
    "model.to(device)\n",
    "\n",
    "model_path= \"./SAVED_MODELS_2024/Model3/Model3_epoch_2_2024_06_09_9.2786.pth\"\n",
    "model.load_state_dict(torch.load(model_path),strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a CSV file to store real and generated captions\n",
    "csv_file = './Results2024/Model3/Model3_epoch_2_2024_06_09_9.2786.csv'  # Change the filename as needed\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Real Caption', 'Generated Caption'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_counter = 0  # Initialize batch counter\n",
    "    total_batches = len(test_loader)  # Total number of batches in the test set\n",
    "    for images, captions in tqdm(test_loader, desc='Testing'):\n",
    "        # Move data to the appropriate device\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        outputs = model.generate(images)  # Assuming your model generates captions given images\n",
    "        outputs = outputs.argmax(dim=2)  # Get the token indices with the highest probability\n",
    "        \n",
    "        if batch_counter % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                generated_output_ids = outputs[0].tolist()\n",
    "                generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids if token_id != 0]\n",
    "                generated_caption_str = \" \".join(generated_words)\n",
    "                real_caption_ids = captions[0].tolist()\n",
    "                real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids if token_id != 0]\n",
    "                real_caption_str = \" \".join(real_caption_words)\n",
    "                print(f\"Generated Output: {generated_caption_str}\")\n",
    "                print(f\"Real Caption: {real_caption_str}\")\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            generated_output_ids = outputs[i].tolist()\n",
    "            generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids if token_id != 0]\n",
    "            generated_caption_str = \" \".join(generated_words)\n",
    "            real_caption_ids = captions[i].tolist()\n",
    "            real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids if token_id != 0]\n",
    "            real_caption_str = \" \".join(real_caption_words)\n",
    "\n",
    "            # Append the real and generated captions to the CSV file\n",
    "            with open(csv_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([real_caption_str, generated_caption_str])\n",
    "        batch_counter += 1\n",
    "\n",
    "print(\"Testing completed. Real and generated captions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = './Results2024/Model3/Model3_epoch_2_2024_06_09_9.2786.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Lists to store metric scores\n",
    "bleu_scores = []\n",
    "rouge_l_scores = []\n",
    "\n",
    "# Smoothing function for BLEU score\n",
    "smoother = SmoothingFunction().method1\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    real_caption = \" \".join(word_tokenize(row['Real Caption']))  # Join the words into a string\n",
    "    generated_caption = \" \".join(word_tokenize(row['Generated Caption']))  # Join the words into a string\n",
    "\n",
    "    real_caption_tokenized = word_tokenize(real_caption)\n",
    "    generated_caption_tokenized = word_tokenize(generated_caption)\n",
    "    \n",
    "    # BLEU Score\n",
    "    bleu = sentence_bleu([real_caption], generated_caption, smoothing_function=smoother)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # ROUGE-L Score\n",
    "    rouge_scores = scorer.score(real_caption, generated_caption)\n",
    "    rouge_l_f1 = rouge_scores['rougeL'].fmeasure\n",
    "    rouge_l_scores.append(rouge_l_f1)\n",
    "\n",
    "# Add metric columns to the DataFrame\n",
    "df['BLEU Score'] = bleu_scores\n",
    "df['ROUGE-L Score'] = rouge_l_scores\n",
    "\n",
    "# Calculate mean scores\n",
    "mean_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "mean_rouge_l_score = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "# Print the DataFrame with scores and mean scores\n",
    "print(df)\n",
    "print(\"\\nMean Metrics:\")\n",
    "print(f\"Mean BLEU Score: {mean_bleu_score:.4f}\")\n",
    "print(f\"Mean ROUGE-L Score: {mean_rouge_l_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "model_path= \"./SAVED_MODELS/model2/ModelEncoder2_epoch_4_Loss_2.2639.pth\"\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "learning_rate = 0.001\n",
    "max_caption_length = dataset.maxCaptionLength\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(embed_size, hidden_size, vocab_size, dataset.maxCaptionLength, num_layers, embed_size, device)  # Remove the duplicated hidden_size argument\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_image_cv2(image_path):\n",
    "    # Load and preprocess the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to (224, 224)\n",
    "    image = image / 255.0  # Normalize pixel values to the range [0, 1]\n",
    "    image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "# Function to load and preprocess the image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "def generate_caption(model, image_path, id_to_token, real_description=None):\n",
    "    # Load and preprocess the image using OpenCV\n",
    "    image = load_and_preprocess_image(image_path)\n",
    "\n",
    "    # Move the model and image to the same device\n",
    "    device = next(model.parameters()).device\n",
    "    image = image.to(device)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the generated tokens and attention scores\n",
    "        generated_tokens, attention_scores = model.generate(image)\n",
    "\n",
    "        # Convert token IDs to words\n",
    "        generated_words = [id_to_token.get(token_id.item(), \"UNK\") for token_id in generated_tokens[0] if token_id != 0]\n",
    "        generated_caption_str = \" \".join(generated_words)\n",
    "\n",
    "    print(\"Generated description by the model:\", \"###  \", generated_caption_str, \"  ###\")\n",
    "    if real_description:\n",
    "        print(f\"Real description made by hand by the tester: ###  {real_description}  ###\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop over each image in the folder\n",
    "folder_path = './ImagenesPrueba/'\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename:#.endswith(\".jpg\")\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Print the image filename\n",
    "        print(f\"Image: {filename}\")\n",
    "\n",
    "        # Use the generate_caption function\n",
    "        generate_caption(model, image_path, dataset.id_to_token)\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
