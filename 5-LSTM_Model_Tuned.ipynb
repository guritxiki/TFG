{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from  customDatasetFromCSV import CustomDatasetFromCSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_csv= './filesCSV/captions.csv'\n",
    "vocab_csv ='./filesCSV/vocab.csv'\n",
    "data_dir = \"./imagesTrainVal/train2017/\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization --> For making the models more robust to low quality images\n",
    "])\n",
    "dataset = CustomDatasetFromCSV(data_dir,captions_csv,vocab_csv,transform=transform,percentage=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capacidad de la GPU:\", torch.cuda.get_device_capability(0))\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST IF THE DATASET IS CORRECT AND IF THE METHODS ARE WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dataset.__len__())\n",
    "print(\"Vocab: \",dataset.vocab)\n",
    "print(f'all captions are the same size: {dataset.allCaptionsSameSize()}')\n",
    "train_item_caption= dataset.__getitem__(43)[1]\n",
    "print(f'We get a random caption of the train:Â´{train_item_caption} ')\n",
    "print(f'Print first ten captions')\n",
    "for i in range(1,11):\n",
    "    print(dataset.get_caption(i))\n",
    "    print(dataset.get_text_caption(i))\n",
    "dataset.print_image_with_caption(45677)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I CRETE THE ENCODER-DECODER ARQUITECTURE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size,device):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        from torchvision.models.resnet import resnet50\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        self.device = device\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules).to(self.device)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size).to(self.device)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "# --------- Decoder ----------\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size,caption_lenght ,num_layers,device):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers= num_layers\n",
    "        self.caption_lenght = caption_lenght\n",
    "        self.hidden = None\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.embed_size= embed_size\n",
    "        # Embedding layer to convert token IDs to embeddings\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size).to(self.device)\n",
    "        # LSTM layer that takes embeddings and hidden states as input\n",
    "        self.lstm = nn.LSTM(embed_size*2, hidden_size, num_layers, batch_first=True).to(self.device)\n",
    "        # Linear layer to produce the vocabulary distribution\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size).to(self.device)\n",
    "    # Initialize values for hidden and cell states    \n",
    "    def init_hidden(self, batch_size,device):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state.\n",
    "        \"\"\"\n",
    "        # Create initial hidden and cell states filled with zeros\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim,device=device),\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        )\n",
    "    def forward(self, features, captions):\n",
    "        # Embed the start token\n",
    "        input_token = torch.tensor([0], device=self.device) \n",
    "        input_token = input_token.expand(features.size(0)).unsqueeze(1)\n",
    "        # Initialize the hidden state for the beginning of the sequence\n",
    "        hidden = self.init_hidden(features.size(0), self.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.caption_lenght):\n",
    "            embeddings = self.embed(input_token)\n",
    "            # print(\"embeddings shape: \"+ str(embeddings.shape))\n",
    "            # Combine the image features and embedded captions as input\n",
    "            inputs = torch.cat((features.unsqueeze(1), embeddings), dim=2)\n",
    "            # print(\"inputs shape: \"+ str(inputs.shape))\n",
    "            # Pass the inputs and hidden states through the LSTM\n",
    "            lstm_out, hidden = self.lstm(inputs, hidden)\n",
    "            # print(\"lstm_out shape: \"+ str(lstm_out.shape))\n",
    "            # Pass the LSTM outputs through the linear layer\n",
    "            output = self.linear(lstm_out)\n",
    "            # print(\"output shape: \"+ str(output.shape))\n",
    "            # Use ground truth token as the input at the next time step\n",
    "            input_token = captions[:, t].unsqueeze(1).to(self.device)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # Concatenate the output tensors along the sequence dimension\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def generate(self, features):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        Args:\n",
    "            features: Output from the encoder (image features)\n",
    "\n",
    "        Returns:\n",
    "            outputs: Predicted token scores (before softmax)\n",
    "        \"\"\"\n",
    "        # Embed the start token\n",
    "        input_token = torch.tensor([0], device=self.device)  # Start token\n",
    "        input_token = input_token.expand(features.size(0)).unsqueeze(1).to(self.device)  # Move to the correct device\n",
    "\n",
    "        # Initialize the hidden state for the beginning of the sequence\n",
    "        hidden = self.init_hidden(features.size(0), self.device)\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(self.caption_lenght):  # Fixed caption length\n",
    "            embeddings = self.embed(input_token)\n",
    "\n",
    "            # Combine the image features and embedded captions as input\n",
    "            inputs = torch.cat((features.unsqueeze(1), embeddings), dim=2)\n",
    "\n",
    "            # Pass the inputs and hidden states through the LSTM\n",
    "            lstm_out, hidden = self.lstm(inputs, hidden)\n",
    "\n",
    "            # Pass the LSTM outputs through the linear layer\n",
    "            output = self.linear(lstm_out)\n",
    "\n",
    "            # Update the input token for the next time step\n",
    "            input_token = output.argmax(2).to(self.device)  # Use argmax to get the next token\n",
    "\n",
    "            # Handle end token and padding token\n",
    "            input_token[input_token == 2] = 0  # Replace end token with padding token\n",
    "            outputs.append(output)\n",
    "            \n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, caption_length, num_layers,device):\n",
    "        super(Model1, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size,device)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, caption_length, num_layers,device)\n",
    "        self.encoder.to(device)\n",
    "        self.decoder.to(device)\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    def generate(self,image):\n",
    "        features= self.encoder(image)\n",
    "        return self.decoder.generate(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# Definition the hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize \n",
    "model = Model1(embed_size, hidden_size, len(dataset.vocab),dataset.maxCaptionLength, num_layers,device)\n",
    "\n",
    "model.encoder.to(device)\n",
    "model.decoder.to(device)\n",
    "model.to(device)\n",
    "\n",
    "# atribbutes of the loop\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Create dataLoaders\n",
    "batch_size = 64  # More for my computer was too bad\n",
    "\n",
    "#data loaders \n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data))) )\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(range(len(dataset.train_data), len(dataset.train_data) + len(dataset.val_data))))\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size*6, sampler=SubsetRandomSampler(range(len(dataset.train_data) + len(dataset.val_data), len(dataset))))\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUMBER OF TRAINABLE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to count the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Make sure to move the model to the correct device\n",
    "model.to(device)\n",
    "\n",
    "# Print the total number of parameters\n",
    "total_params = count_parameters(model)\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK CONSISTENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_device_consistency(model):\n",
    "    def get_device(obj):\n",
    "        return obj.device if hasattr(obj, \"device\") else None\n",
    "\n",
    "    def check_device_match(device_list):\n",
    "        return all(device == device_list[0] for device in device_list)\n",
    "\n",
    "    param_devices = [get_device(param) for param in model.parameters()]\n",
    "    buffer_devices = [get_device(buf) for buf in model.buffers()]\n",
    "\n",
    "    all_devices = param_devices + buffer_devices\n",
    "    is_consistent = check_device_match(all_devices)\n",
    "\n",
    "    return is_consistent\n",
    "\n",
    "# Call the function to check device consistency\n",
    "is_device_consistent = check_device_consistency(model)\n",
    "\n",
    "if is_device_consistent:\n",
    "    print(\"All parameters and buffers are on the same device.\")\n",
    "else:\n",
    "    print(\"Parameters and buffers are on different devices.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_model(search_for_existing_model=False):\n",
    "    # Define the directory where you want to save the model\n",
    "    save_dir = 'SAVED_MODELS_2024/model5'\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize variables for tracking the best model and best validation loss\n",
    "    best_model = None\n",
    "    best_val_loss = float('inf')\n",
    "    start_epoch = 0  # Start epoch counter\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        batch_counter = 0  # Initialize batch counter\n",
    "        total_batches = len(train_loader)  # Total number of batches in the training set\n",
    "        train_losses = []\n",
    "\n",
    "        for images, captions in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "            # Move data to the appropriate device (GPU or CPU)\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, captions)  # Exclude the last word for input\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = F.cross_entropy(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            batch_counter += 1\n",
    "\n",
    "            # Print loss and generated output every 50 batches\n",
    "            if batch_counter % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    print(\"################### TRAINING METHOD ################### \")\n",
    "                    generated_output_ids = outputs[0].argmax(dim=1).tolist()\n",
    "                    generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids if token_id != 0]\n",
    "                    generated_caption_str = \" \".join(generated_words)\n",
    "                    real_caption_ids = captions[0].tolist()\n",
    "                    real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids if token_id != 0]\n",
    "                    real_caption_str = \" \".join(real_caption_words)\n",
    "                    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Batch [{batch_counter}/{total_batches}] - Loss: {loss.item():.4f}\")\n",
    "                    print(f\"Generated Output: {generated_caption_str}\")\n",
    "                    print(f\"Real Caption: {real_caption_str}\")\n",
    "            if batch_counter % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    print(\"################### GENERATE METHOD ################### \")\n",
    "                                \n",
    "                    image = images[0].cpu().numpy()  #\n",
    "                    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "                    plt.imshow(image)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    plt.show()\n",
    "                    generated_outputs = model.generate(images)\n",
    "                    generated_output_ids = generated_outputs.argmax(dim=2).tolist()\n",
    "                    generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids[0] if token_id != 0]\n",
    "                    generated_caption_str = \" \".join(generated_words)\n",
    "                    real_caption_ids = captions.tolist()\n",
    "                    real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids[0] if token_id != 0]\n",
    "                    real_caption_str = \" \".join(real_caption_words)\n",
    "                    print(f\"Generated Output (Generated): {generated_caption_str}\")\n",
    "                    print(f\"Real Caption: {real_caption_str}\")\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "        \n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for images, captions in val_loader:\n",
    "                # Move data to the appropriate device\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images, captions)\n",
    "                val_loss = F.cross_entropy(outputs.view(-1, vocab_size), captions.contiguous().view(-1))\n",
    "                val_losses.append(val_loss.item())\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        # Save the model after each epoch\n",
    "        model_path = os.path.join(save_dir, f'model_epoch_{epoch + 1}_11_06_2024_val_loss_{avg_val_loss}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        \n",
    "    print(\"Training completed.\")\n",
    "\n",
    "train_model(search_for_existing_model=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOOSING THE TRAINED MODEL TO TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "model = Model1(embed_size, hidden_size, len(dataset.vocab),dataset.maxCaptionLength, num_layers,device)\n",
    "model.encoder.to(device)\n",
    "model.decoder.to(device)\n",
    "model.to(device)\n",
    "\n",
    "model_path= \"./SAVED_MODELS_2024/Model5/model_epoch_1_11_06_2024_val_loss_0.5446846502536052.pth\"\n",
    "model.load_state_dict(torch.load(model_path),strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a CSV file to store real and generated captions\n",
    "csv_file = './Results2024/ModeL5/model_epoch_1_11_06_2024_val_loss_0.5446846502536052.pth'\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Real Caption', 'Generated Caption'])\n",
    "\n",
    "# Testing loop on the test dataset\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_counter = 0  # Initialize batch counter\n",
    "    total_batches = len(test_loader)  # Total number of batches in the training set\n",
    "    for images, captions in tqdm(test_loader, desc='Testing'):\n",
    "        # Move data to the appropriate device\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        outputs = model.generate(images)  # Assuming your model generates captions given images\n",
    "        if batch_counter % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                generated_output_ids = outputs[0].argmax(dim=1).tolist()\n",
    "                generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids if token_id != 0]\n",
    "                generated_caption_str = \" \".join(generated_words)\n",
    "                real_caption_ids = captions[0].tolist()\n",
    "                real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids if token_id != 0]\n",
    "                real_caption_str = \" \".join(real_caption_words)\n",
    "                print(f\"Generated Output: {generated_caption_str}\")\n",
    "                print(f\"Real Caption: {real_caption_str}\")\n",
    "        for i in range(len(images)):\n",
    "            generated_output_ids = outputs[i].argmax(dim=1).tolist()\n",
    "            generated_words = [dataset.id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids if token_id != 0]\n",
    "            generated_caption_str = \" \".join(generated_words)\n",
    "            real_caption_ids = captions[i].tolist()\n",
    "            real_caption_words = [dataset.id_to_token.get(int(token_id), '<UNK>') for token_id in real_caption_ids if token_id != 0]\n",
    "            real_caption_str = \" \".join(real_caption_words)\n",
    "\n",
    "            # Append the real and generated captions to the CSV file\n",
    "            with open(csv_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([real_caption_str, generated_caption_str])\n",
    "        batch_counter += 1\n",
    "print(\"Testing completed. Real and generated captions saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METRICS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = './Results2024/Model5/model_epoch_1_11_06_2024_val_loss_0.5446846502536052.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Lists to store metric scores\n",
    "bleu_scores = []\n",
    "rouge_l_scores = []\n",
    "\n",
    "# Smoothing function for BLEU score\n",
    "smoother = SmoothingFunction().method1\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    real_caption = \" \".join(word_tokenize(row['Real Caption']))  # Join the words into a string\n",
    "    generated_caption = \" \".join(word_tokenize(row['Generated Caption']))  # Join the words into a string\n",
    "\n",
    "    real_caption_tokenized = word_tokenize(real_caption)\n",
    "    generated_caption_tokenized = word_tokenize(generated_caption)\n",
    "    \n",
    "    # BLEU Score\n",
    "    bleu = sentence_bleu([real_caption], generated_caption, smoothing_function=smoother)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # ROUGE-L Score\n",
    "    rouge_scores = scorer.score(real_caption, generated_caption)\n",
    "    rouge_l_f1 = rouge_scores['rougeL'].fmeasure\n",
    "    rouge_l_scores.append(rouge_l_f1)\n",
    "\n",
    "# Add metric columns to the DataFrame\n",
    "df['BLEU Score'] = bleu_scores\n",
    "df['ROUGE-L Score'] = rouge_l_scores\n",
    "\n",
    "# Calculate mean scores\n",
    "mean_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "mean_rouge_l_score = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "# Print the DataFrame with scores and mean scores\n",
    "print(df)\n",
    "print(\"\\nMean Metrics:\")\n",
    "print(f\"Mean BLEU Score: {mean_bleu_score:.4f}\")\n",
    "print(f\"Mean ROUGE-L Score: {mean_rouge_l_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "model_path= \"./SAVED_MODELS/model1/best_model_epoch_5.pth\"\n",
    "# Define the hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "vocab = dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize the model\n",
    "model = Model1(embed_size, hidden_size, len(dataset.vocab),dataset.maxCaptionLength, num_layers,device)\n",
    "\n",
    "model.encoder.to(device)\n",
    "model.decoder.to(device)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_image_cv2(image_path):\n",
    "    # Load and preprocess the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to (224, 224)\n",
    "    image = image / 255.0  # Normalize pixel values to the range [0, 1]\n",
    "    image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "# Function to load and preprocess the image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "def generate_caption(model, image_path, id_to_token, real_description=None):\n",
    "    # Load and preprocess the image using OpenCV\n",
    "    image = load_and_preprocess_image(image_path)\n",
    "\n",
    "    # Move the model and image to the same device\n",
    "    device = next(model.parameters()).device\n",
    "    image = image.to(device)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(image)\n",
    "        # Use argmax(dim=2) to get the index of the predicted token\n",
    "        generated_output_ids = output.argmax(dim=2).squeeze().tolist()\n",
    "        generated_words = [id_to_token.get(token_id, \"UNK\") for token_id in generated_output_ids if token_id != 0]\n",
    "        generated_caption_str = \" \".join(generated_words)\n",
    "\n",
    "    print(\"Generated description by the model: \", \"###  \", generated_caption_str, \"  ###\")\n",
    "    if real_description:\n",
    "        print(f\"Real description made by hand by the tester: ###  {real_description}  ###\")\n",
    "\n",
    "# Example usage:\n",
    "generate_caption(model, './ImagenesPrueba/20220511_155217.jpg', dataset.id_to_token, real_description=\"A man taking an image at a bathroom\")\n",
    "generate_caption(model, './ImagenesPrueba/20230413_031752.jpg', dataset.id_to_token,real_description=\" A man taking an image at a bathroom\")\n",
    "\n",
    "\n",
    "\n",
    "# Loop over each image in the folder\n",
    "folder_path = './ImagenesPrueba/'\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename:#.endswith(\".jpg\")\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Print the image filename\n",
    "        print(f\"Image: {filename}\")\n",
    "\n",
    "        # Use the generate_caption function\n",
    "        generate_caption(model, image_path, dataset.id_to_token)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
